import json
import subprocess
import time
import os
from tqdm import tqdm

# è®¾ç½®è·¯å¾„
DATASET_PATH = "./data/cruxeval_sample.jsonl"
GENERATIONS_DIR = "./model_generations/your_model_name/generations/output"

PROMPT_TEMPLATE = """Below is an instruction that describes a task, paired with an input and output. Write a Python function to solve the task.

### Instruction:
{instruction}

### Input:
{input}

### Response:"""

def load_cruxeval_dataset(path):
    """åŠ è½½ cruxeval.jsonl æ•°æ®é›†"""
    dataset = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            dataset.append(json.loads(line.strip()))
    return dataset


def generate_with_ollama(prompt, model_name="llama3", max_tokens=200, temperature=0.2):
    """
    è°ƒç”¨ ollama ç”Ÿæˆå“åº”
    :param prompt: æç¤ºè¯
    :param model_name: æ¨¡å‹å
    :param max_tokens: æœ€å¤§è¾“å‡º token æ•°
    :param temperature: æ¸©åº¦å‚æ•°
    :return: æ¨¡å‹è¾“å‡ºæ–‡æœ¬
    """
    cmd = [
        "ollama",
        "run",
        model_name,
        prompt
    ]

    try:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
            timeout=60,
            check=True,
        )
        return result.stdout.strip()
    except Exception as e:
        print(f"âš ï¸ Ollama ç”Ÿæˆå¤±è´¥: {e}")
        return ""


def build_prompt(sample):
    """æ ¹æ® sample æ„å»º Alpaca æ ¼å¼ prompt"""
    return PROMPT_TEMPLATE.format(
        instruction=sample["nl"],
        input=str(sample["input"]),
    )


def main(model_name="codellama", temperature=0.2):
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(GENERATIONS_DIR, exist_ok=True)

    # åŠ è½½æ•°æ®é›†
    dataset = load_cruxeval_dataset(DATASET_PATH)
    print(f"âœ… åŠ è½½äº† {len(dataset)} æ¡æµ‹è¯•æ ·æœ¬")

    generations = {}
    for idx, sample in enumerate(tqdm(dataset, desc="ğŸ§  Generating")):
        prompt = build_prompt(sample)

        # è°ƒç”¨ ollama ç”Ÿæˆ
        response = generate_with_ollama(prompt, model_name=model_name)

        generations[f"sample_{idx}"] = [response]  # æ”¯æŒå¤š generationï¼Œè¿™é‡Œåªä¿ç•™ä¸€ä¸ª

        # å¯é€‰ï¼šæ§åˆ¶é€Ÿç‡
        time.sleep(0.1)

    # ä¿å­˜ç”Ÿæˆç»“æœ
    output_path = os.path.join(GENERATIONS_DIR, f"{model_name}_temp{temperature}_output.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(generations, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç”Ÿæˆå®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_path}")


if __name__ == "__main__":
    main(model_name="deepseek-coder:6.7b", temperature=0.2)